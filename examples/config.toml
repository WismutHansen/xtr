# XTR structured extraction engine example configuration.
# Copy this file to $XDG_CONFIG_HOME/xtr/config.toml (or ~/.config/xtr/config.toml)
# to customize defaults. Values support environment variable expansion like
# $XDG_CONFIG_HOME and home expansion via ~.

[logging]
# Enable verbose logging of complete request-response cycle with the LLM (default: false)
# When enabled, all LLM requests and responses will be logged to JSON files
verbose_llm_logging = false

# Directory where LLM request-response logs will be written as JSON files
# Defaults to $XDG_STATE_HOME/xtr/llm_logs if not specified
# llm_log_dir = "~/.local/state/xtr/llm_logs"

[metrics]
# Configuration for evaluation metrics used during optimization.
# These control how extraction quality is scored.

# Weight for extra/hallucinated fields (0.0-1.0, default: 0.5)
# Lower values = lighter penalty for hallucinations vs wrong values
extra_field_weight = 0.5

# Beta parameter for F-beta score (default: 1.5)
# Values > 1.0 favor recall over precision, < 1.0 favor precision
# 1.5 means recall is 1.5x more important than precision
beta = 1.5

# Base score awarded for valid JSON parsing (default: 0.2)
base_parse_score = 0.2

# Additional score for schema validation (default: 0.2)
base_schema_score = 0.2

# Weight for field-level quality (F-beta of precision/recall) (default: 0.5)
field_weight = 0.5

# Weight for coverage bonus (encourages completeness) (default: 0.1)
coverage_weight = 0.1

[models.defaults.teacher]
name = "llama-teacher"
base_url = "http://localhost:8080/v1"
api_key = "not_needed"
adapter = "chat"

[models.defaults.student]
name = "llama-student"
base_url = "http://localhost:8081/v1"
api_key = "not_needed"
adapter = "chat"

[[models.defaults.fallbacks]]
name = "llama-backup"
base_url = "http://localhost:8082/v1"
api_key = "not_needed"
adapter = "chat"

# Optional per-task overrides. Keys correspond to task identifiers.
[models.tasks."invoice_extraction"]
teacher.name = "mixtral-teacher"
teacher.base_url = "http://localhost:8090/v1"
student.name = "mxb-student"
student.base_url = "http://localhost:8091/v1"

[[models.tasks."invoice_extraction".fallbacks]]
name = "mixtral-guard"
base_url = "http://localhost:8092/v1"

[storage]
# Override storage directories if desired; otherwise defaults use XDG env vars.
# data_dir = "~/Library/Application Support/xtr"
# state_dir = "~/.local/state/xtr"
# cache_dir = "$XDG_STATE_HOME/xtr/cache"

[data_collection]
# Collect inference inputs/outputs as JSON examples (default: false)
enabled = true
output_dir = "$XDG_STATE_HOME/xtr/collected_examples"

[mlflow]
# MLflow experiment tracking (optional). Enable the 'mlflow' feature to use this.
# tracking_uri = "http://localhost:5000"
# experiment_name = "xtr-optimization"

# Local logging always writes structured logs to disk (enabled by default)
local_logging = true
# log_dir = "~/.local/state/xtr/optimization_logs"  # Optional, defaults to XDG_STATE_HOME/xtr/optimization_logs

[optimization.defaults]
# Number of evolutionary iterations (default: 4)
iterations = 4

# Number of trials per iteration for averaging stochastic LM behavior (default: 6)
rollouts_per_iteration = 6

# Maximum total language model API calls budget (default: 32)
max_lm_calls = 32

# Batch size for evaluation - larger = better signal but slower (default: 4)
batch_size = 4

# Maximum total rollouts budget - hard constraint on evaluations (optional, no default)
# max_rollouts = 100

# Temperature for LLM-based mutations (default: 0.9)
# Higher values (>1.0) = more creative/exploratory mutations
# Lower values (<1.0) = more conservative mutations
# Range: 0.0-2.0
temperature = 0.9

# Track detailed optimization statistics (default: true)
# Set to false to reduce memory overhead
track_stats = true

# Track best outputs for inference-time search (default: false)
# Enable to store best outputs found during evolution
track_best_outputs = false

[optimization.defaults.feedback_models.teacher]
name = "llama-feedback"
base_url = "http://localhost:8085/v1"

[optimization.tasks."invoice_extraction"]
iterations = 6
rollouts_per_iteration = 8
max_lm_calls = 48
# You can override any optimization parameter here
# temperature = 1.2
# max_rollouts = 200

[tasks."invoice_extraction"]
schema = "~/data/xtr/schemas/invoice.json"
examples = "~/data/xtr/examples/invoice"
description = "Extract structured invoice metadata from scanned PDFs."
# include_timestamp adds current date/time to the model's context (default: false)
include_timestamp = false

[tasks."troubleshooting_report"]
schema = "$XDG_DATA_HOME/xtr/schemas/troubleshooting.json"
examples = "$XDG_DATA_HOME/xtr/examples/troubleshooting"
description = "Summarize troubleshooting steps and outcomes from field reports."
# include_timestamp adds current date/time to the model's context (default: false)
include_timestamp = true
